{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "r094Dy18OMsg",
        "RP0iY45PtSFW",
        "rex6xev5qP-G",
        "pUYfZKNlhSFn"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prerequisites"
      ],
      "metadata": {
        "id": "r094Dy18OMsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1. Install and initialize libraries and constants"
      ],
      "metadata": {
        "id": "RP0iY45PtSFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Training Outputs\n",
        "Google colabs has no persistant memory, thus it needs to be stored and retrieved on Google Drive"
      ],
      "metadata": {
        "id": "U4WKT_5pxkce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown 16vZmWPgR2wPPG_nvVwlxhwUIac-OgQeW\n",
        "!unzip Training_results.zip"
      ],
      "metadata": {
        "id": "TJFfhi5rfQvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Environment\n",
        "I'll be using Keras/Tensorflow for the age-classification, and OpenCV for webcam data-handling and face identification"
      ],
      "metadata": {
        "id": "_EM-olJEyKal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from base64 import b64decode, b64encode\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import io\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "IMG_SHAPE = [640, 480]\n",
        "IMG_QUALITY = 0.8\n",
        "\n",
        "model = load_model(\"/content/Training_results/MPAA_cnn_model_checkpoint.keras\") # My age-classification model, see CS Capstone Training.ipynb for more detail\n",
        "age_ranges = ['PG', 'PG13', 'R']\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') #helper model to locate faces to feed into my model."
      ],
      "metadata": {
        "id": "XAgO_ab-roQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rex6xev5qP-G"
      },
      "source": [
        "### 0.2. JavaScript workaround to grab live webcam stream\n",
        "Google colabs is a cloud based environment and can't see any of the attached physical devices. As such, a JS workaround to capture the webcam is necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OYmjeF-edKE"
      },
      "source": [
        "def start_stream():\n",
        "    js = Javascript(f'''\n",
        "    const IMG_SHAPE = {IMG_SHAPE};\n",
        "    const IMG_QUALITY = {IMG_QUALITY};\n",
        "    ''' + '''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        video.remove();\n",
        "        div.remove();\n",
        "        video = null;\n",
        "        div = null;\n",
        "        stream = null;\n",
        "        imgElement = null;\n",
        "        captureCanvas = null;\n",
        "        labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "        if (!shutdown) {\n",
        "            window.requestAnimationFrame(onAnimationFrame);\n",
        "        }\n",
        "        if (pendingResolve) {\n",
        "            var result = \"\";\n",
        "            if (!shutdown) {\n",
        "                captureCanvas.getContext('2d').drawImage(video, 0, 0, IMG_SHAPE[0], IMG_SHAPE[1]);\n",
        "                result = captureCanvas.toDataURL('image/jpeg', IMG_QUALITY)\n",
        "            }\n",
        "            var lp = pendingResolve;\n",
        "            pendingResolve = null;\n",
        "            lp(result);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "        if (div !== null) {\n",
        "            return stream;\n",
        "        }\n",
        "\n",
        "        div = document.createElement('div');\n",
        "        div.style.border = '2px solid black';\n",
        "        div.style.padding = '3px';\n",
        "        div.style.width = '100%';\n",
        "        div.style.maxWidth = '600px';\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        const modelOut = document.createElement('div');\n",
        "        modelOut.innerHTML = \"<span>Status: </span>\";\n",
        "        labelElement = document.createElement('span');\n",
        "        labelElement.innerText = 'No data';\n",
        "        labelElement.style.fontWeight = 'bold';\n",
        "        modelOut.appendChild(labelElement);\n",
        "        div.appendChild(modelOut);\n",
        "\n",
        "        video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        video.width = div.clientWidth - 6;\n",
        "        video.setAttribute('playsinline', '');\n",
        "        video.onclick = () => { shutdown = true; };\n",
        "        stream = await navigator.mediaDevices.getUserMedia(\n",
        "            {video: { facingMode: \"environment\"}});\n",
        "        div.appendChild(video);\n",
        "\n",
        "        imgElement = document.createElement('img');\n",
        "        imgElement.style.position = 'absolute';\n",
        "        imgElement.style.zIndex = 1;\n",
        "        imgElement.onclick = () => { shutdown = true; };\n",
        "        div.appendChild(imgElement);\n",
        "\n",
        "        const instruction = document.createElement('div');\n",
        "        instruction.innerHTML =\n",
        "            '<span style=\"color: red; font-weight: bold;\">' +\n",
        "            'When finished, click here or on the video to stop this demo</span>';\n",
        "        div.appendChild(instruction);\n",
        "        instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        captureCanvas = document.createElement('canvas');\n",
        "        captureCanvas.width = IMG_SHAPE[0]; //video.videoWidth;\n",
        "        captureCanvas.height = IMG_SHAPE[1]; //video.videoHeight;\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "        return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "        if (shutdown) {\n",
        "            removeDom();\n",
        "            shutdown = false;\n",
        "            return '';\n",
        "        }\n",
        "\n",
        "        var preCreate = Date.now();\n",
        "        stream = await createDom();\n",
        "\n",
        "        var preShow = Date.now();\n",
        "        if (label != \"\") {\n",
        "            labelElement.innerHTML = label;\n",
        "        }\n",
        "\n",
        "        if (imgData != \"\") {\n",
        "            var videoRect = video.getClientRects()[0];\n",
        "            imgElement.style.top = videoRect.top + \"px\";\n",
        "            imgElement.style.left = videoRect.left + \"px\";\n",
        "            imgElement.style.width = videoRect.width + \"px\";\n",
        "            imgElement.style.height = videoRect.height + \"px\";\n",
        "            imgElement.src = imgData;\n",
        "        }\n",
        "\n",
        "        var preCapture = Date.now();\n",
        "        var result = await new Promise((resolve, reject) => pendingResolve = resolve);\n",
        "        shutdown = false;\n",
        "\n",
        "        return {\n",
        "            'create': preShow - preCreate,\n",
        "            'show': preCapture - preShow,\n",
        "            'capture': Date.now() - preCapture,\n",
        "            'img': result,\n",
        "        };\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def take_photo(label, img_data):\n",
        "    data = eval_js(f'takePhoto(\"{label}\", \"{img_data}\")')\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3. Define Python functions to interpret webcam stream, classify detected faces, and create the prediction overlay\n",
        "OpenCV reduces the framerate considerably, as does the Keras age-classification. To reduce the annoyingly choppy framerate I've created a transparent overlay for the prediction results. The transparent overlay improves the performance a bit, but is still noticably choppy compared to the webcam stream."
      ],
      "metadata": {
        "id": "pUYfZKNlhSFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBHXYL0agvtW"
      },
      "outputs": [],
      "source": [
        "def js_response_to_image(js_response) :\n",
        "    _, b64_str = js_response['img'].split(',')\n",
        "    jpeg_bytes = b64decode(b64_str)\n",
        "    im_arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)  # im_arr is one-dim Numpy array\n",
        "    img = cv2.imdecode(im_arr, flags=cv2.IMREAD_COLOR)\n",
        "\n",
        "    #image = np.array(jpeg_bytes,dtype=\"uint8\")\n",
        "    return img\n",
        "# Defining a function to shrink the detected face region by a scale for better prediction in the model.\n",
        "\n",
        "def shrink_face_roi(x, y, w, h, scale=0.9):\n",
        "    wh_multiplier = (1-scale)/2\n",
        "    x_new = int(x + (w * wh_multiplier))\n",
        "    y_new = int(y + (h * wh_multiplier))\n",
        "    w_new = int(w * scale)\n",
        "    h_new = int(h * scale)\n",
        "    return (x_new, y_new, w_new, h_new)\n",
        "# Defining a function to create the predicted age overlay on the image by centering the text.\n",
        "\n",
        "def create_age_text(img, text, pct_text, x, y, w, h):\n",
        "\n",
        "    # Defining font, scales and thickness.\n",
        "    fontFace = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    text_scale = 1.2\n",
        "    yrsold_scale = 0.7\n",
        "    pct_text_scale = 0.65\n",
        "\n",
        "    # Getting width, height and baseline of age text and \"years old\".\n",
        "    (text_width, text_height), text_bsln = cv2.getTextSize(text, fontFace=fontFace, fontScale=text_scale, thickness=2)\n",
        "    (yrsold_width, yrsold_height), yrsold_bsln = cv2.getTextSize(\"years old\", fontFace=fontFace, fontScale=yrsold_scale, thickness=1)\n",
        "    (pct_text_width, pct_text_height), pct_text_bsln = cv2.getTextSize(pct_text, fontFace=fontFace, fontScale=pct_text_scale, thickness=1)\n",
        "\n",
        "    # Calculating center point coordinates of text background rectangle.\n",
        "    x_center = x + (w/2)\n",
        "    y_text_center = y + h + 20\n",
        "    y_yrsold_center = y + h + 48\n",
        "    y_pct_text_center = y + h + 75\n",
        "\n",
        "    # Calculating bottom left corner coordinates of text based on text size and center point of background rectangle calculated above.\n",
        "    x_text_org = int(round(x_center - (text_width / 2)))\n",
        "    y_text_org = int(round(y_text_center + (text_height / 2)))\n",
        "    x_yrsold_org = int(round(x_center - (yrsold_width / 2)))\n",
        "    y_yrsold_org = int(round(y_yrsold_center + (yrsold_height / 2)))\n",
        "    x_pct_text_org = int(round(x_center - (pct_text_width / 2)))\n",
        "    y_pct_text_org = int(round(y_pct_text_center + (pct_text_height / 2)))\n",
        "\n",
        "    face_age_background = cv2.rectangle(img, (x-1, y+h), (x+w+1, y+h+94), (0, 100, 0, 255), cv2.FILLED)\n",
        "    face_age_text = cv2.putText(img, text, org=(x_text_org, y_text_org), fontFace=fontFace, fontScale=text_scale, thickness=2, color=(255, 255, 255,255), lineType=cv2.LINE_AA)\n",
        "    yrsold_text = cv2.putText(img, \"Max Rating\", org=(x_yrsold_org, y_yrsold_org), fontFace=fontFace, fontScale=yrsold_scale, thickness=1, color=(255, 255, 255,255), lineType=cv2.LINE_AA)\n",
        "    pct_age_text = cv2.putText(img, pct_text, org=(x_pct_text_org, y_pct_text_org), fontFace=fontFace, fontScale=pct_text_scale, thickness=1, color=(255, 255, 255,255), lineType=cv2.LINE_AA)\n",
        "\n",
        "    return (face_age_background, face_age_text, yrsold_text)\n",
        "    # Defining a function to find faces in an image and then classify each found face into age-ranges defined above.\n",
        "\n",
        "def classify_age(img):\n",
        "\n",
        "\n",
        "    blank_image = np.zeros((480,640,4), np.uint8)\n",
        "    # Detecting faces in the image using the face_cascade loaded above and storing their coordinates into a list.\n",
        "    faces = face_cascade.detectMultiScale(img, scaleFactor=1.2, minNeighbors=6, minSize=(100, 100))\n",
        "    #print(\"{faces} faces found.\")\n",
        "\n",
        "    # Looping through each face found in the image.\n",
        "    for i, (x, y, w, h) in enumerate(faces):\n",
        "\n",
        "        # Drawing a rectangle around the found face.\n",
        "        face_rect = cv2.rectangle(blank_image, (x, y), (x+w, y+h), (0, 100, 0, 255), thickness=2)\n",
        "\n",
        "        # Predicting the age of the found face using the model loaded above.\n",
        "        x2, y2, w2, h2 = shrink_face_roi(x, y, w, h)\n",
        "        face_roi = img[y2:y2+h2, x2:x2+w2]\n",
        "        face_roi = cv2.resize(face_roi, (200, 200))\n",
        "        face_roi = face_roi.reshape(-1, 200, 200, 3)\n",
        "        face_age = age_ranges[np.argmax(model.predict(face_roi, verbose=0))]\n",
        "        face_age_pct = f\"({round(np.max(model.predict(face_roi, verbose=0))*100, 2)}%)\"\n",
        "\n",
        "        # Calling the above defined function to create the predicted age overlay on the image.\n",
        "        face_age_background, face_age_text, yrsold_text = create_age_text(blank_image, face_age, face_age_pct, x, y, w, h)\n",
        "\n",
        "        # print(f\"Age prediction for face {i+1} : {face_age} years old\")\n",
        "\n",
        "    return blank_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnlBsUw41urd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIlKEQ3ruDCw"
      },
      "source": [
        "## 1. Perform real-time object detection in webcam stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZi6NXSDyAY6"
      },
      "source": [
        "start_stream()\n",
        "img_data = ''\n",
        "while True:\n",
        "    js_response = take_photo('Capturing...Overlay may be choppy', img_data)\n",
        "    if not js_response:\n",
        "        break\n",
        "    captured_img = js_response_to_image(js_response)\n",
        "    result = classify_age(captured_img)\n",
        "    img_1 = cv2.imencode('.png', result)\n",
        "    buf = io.BytesIO(img_1[1])\n",
        "    img_64 = b64encode(buf.getvalue())\n",
        "    img_str = img_64.decode('utf-8')\n",
        "    img_data = 'data:image/png;base64,' + img_str"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}